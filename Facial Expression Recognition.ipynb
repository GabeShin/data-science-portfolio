{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Facial Expression Recognition Library\n",
    "___\n",
    "\n",
    "In this project, I will aspired to libraries such as Sci_kit learn and create a plug-in neural network library\n",
    "\n",
    "This was a Kaggle competition from 2013. I used the data to practice different models on the data. Following is overview of the competition and the data:\n",
    "\n",
    "\n",
    "    One motivation for representation learning is that learning algorithms can design features better and faster than humans can. To this end, we hold this challenge that does not explicitly require that entries use representation learning. Rather, we introduce an entirely new dataset and invite competitors from all related communities to solve it. The dataset for this challenge is a facial expression classification dataset that we have assembled from the internet. Because this is a newly introduced dataset, this contest will see which methods are the easiest to get quickly working on new data.\n",
    "    \n",
    "## Data\n",
    "____\n",
    "\n",
    "    The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "____\n",
    "Comprised of utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias(M1, M2):\n",
    "    # M1: input size\n",
    "    # M2: output size\n",
    "\n",
    "    W = np.random.randn(M1, M2) / np.sqrt(M1 + M2)\n",
    "    # matrix of M1 by M2, randomized initially to Gaussian normal, divided by square root of the fan-in plus fan-out.\n",
    "    b = np.zeros(M2)\n",
    "    # bias initalized as zeros\n",
    "    return W.astype(np.float32), b.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter(shape, poolsz):\n",
    "    # used for convolutional neural network\n",
    "    w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[1:]) + shape[0]*np.prod(shape[2:] / np.prod(poolsz)))\n",
    "    return w.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / expX.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost functions\n",
    "def sigmoid_cost(T, Y):\n",
    "    # calculates the cross entropy from the definition for sigmoid cost/ binary classification\n",
    "    return -(T*np.log(Y) + (1-T)*np.log(1-Y)).sum()\n",
    "\n",
    "def cost(T, Y):\n",
    "    # general cross entropy function, works for softmax\n",
    "    return -(T*np.log(Y)).sum()\n",
    "\n",
    "def cost2(T, Y):\n",
    "    # same as cost(), just uses the targets to index Y\n",
    "    # instead of multiplying by a large indicator matrix with mostly 0s\n",
    "    N = len(T)\n",
    "    return -np.log(Y[np.arange(N), T]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate related!\n",
    "def error_rate(targets, predictions):\n",
    "    return np.mean(targets != predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning related functions\n",
    "\n",
    "def y2indicator(y):\n",
    "    # convert y into indicator matrix\n",
    "    # size will be N by K\n",
    "    N = len(y)\n",
    "    K = len(set(y))\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "def getData(balance_ones=True):\n",
    "    # get facial expression data\n",
    "    \n",
    "    # images are 48x48 = 2304 size vectors\n",
    "    \n",
    "    #initialize empty list for X and Y\n",
    "    Y = []\n",
    "    X = []\n",
    "    \n",
    "    # open data\n",
    "    first = True\n",
    "    for line in open('input/fer2013.csv'):\n",
    "        # skip first line\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            # first column is labels -> y\n",
    "            Y.append(int(row[0]))      \n",
    "            # second column is space separated pixels\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "\n",
    "    # convert these into Numpy array\n",
    "    # and also normalize the data\n",
    "    X = np.array(X) / 255.0\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    # because we have imbalance class problem, we will balance the class 1. \n",
    "    if balance_ones:\n",
    "        # get all data except class 1\n",
    "        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n",
    "        \n",
    "        # get all class 1 data\n",
    "        X1 = X[Y==1, :]\n",
    "        \n",
    "        # repeat the data 9 times\n",
    "        X1 = np.repeat(X1, 9, axis=0)\n",
    "        \n",
    "        # stack the data for X0 and X1  \n",
    "        # stack the data for Y0 and 1\n",
    "        X = np.vstack([X0, X1])\n",
    "        Y = np.concatenate((Y0, [1]*len(X1)))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def getImageData():\n",
    "    # keep image shape\n",
    "    X, Y = getData()\n",
    "    N, D = X.shape\n",
    "    d = int(np.sqrt(D))\n",
    "    X = X.reshape(N, 1, d, d)\n",
    "    return X, Y\n",
    "\n",
    "def getBinaryData():\n",
    "    # same as getData function, except we only get binary data/ Y = 0, 1\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    for line in open('input/fer2013.csv'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            y = int(row[0])\n",
    "            if y == 0 or y == 1:\n",
    "                Y.append(y)\n",
    "                X.append([int(p) for p in row[1].split()])\n",
    "    return np.array(X) / 255.0, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Artificial Neural Network for Binary Classification using ReLU and tanh\n",
    "___\n",
    "Create ANN class for facial expression recognition.\n",
    "- Binary Classification (only between 0 and 1)\n",
    "- One hidden layer\n",
    "- uses relu and tanh as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=5e-6, reg=1.0, epochs=10000, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        N, D = X.shape\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M) / np.sqrt(self.M)\n",
    "        self.b2 = 0\n",
    "\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation and cost calculation\n",
    "            pY, Z = self.forward(X)            \n",
    "\n",
    "            # back propagation - gradient descent step\n",
    "            \n",
    "            # hidden-to-outer\n",
    "            pY_Y = pY - Y # prediction minus the target\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_Y) + reg*self.W2) # update hidden-to-output weight\n",
    "            self.b2 -= learning_rate*((pY_Y).sum() + reg*self.b2) # update hidden-out-output bias\n",
    "\n",
    "            # print \"(pY_Y).dot(self.W2.T) shape:\", (pY_Y).dot(self.W2.T).shape\n",
    "            # print \"Z shape:\", Z.shape\n",
    "\n",
    "            # input-to-hidden\n",
    "            dZ = np.outer(pY_Y, self.W2) * (1 - Z*Z) # tanh\n",
    "            # dZ = np.outer(pY_Y, self.W2) * (Z > 0) # ReLU\n",
    "            \n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1) # update input-to-hidden weight\n",
    "            self.b1 -= learning_rate*(np.sum(dZ, axis=0) + reg*self.b1) # update input-to-hidden bias\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "\n",
    "                c = sigmoid_cost(Yvalid, pYvalid)\n",
    "\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.round(pYvalid))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        print(\"validation score: \", self.score(Xvalid, Yvalid))\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1) # tanh\n",
    "        # Z = relu(X.dot(self.W1) + self.b1) # ReLU\n",
    "\n",
    "        return sigmoid(Z.dot(self.W2) + self.b2), Z\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY = self.forward(X)\n",
    "        return np.round(pY)\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "\n",
    "#     def main():\n",
    "#         X, Y = getBinaryData()\n",
    "\n",
    "#         X0 = X[Y==0, :]\n",
    "#         X1 = X[Y==1, :]\n",
    "#         X1 = np.repeat(X1, 9, axis=0)\n",
    "#         X = np.vstack([X0, X1])\n",
    "#         Y = np.array([0]*len(X0) + [1]*len(X1))\n",
    "\n",
    "#         model = ANN(100)\n",
    "#         model.fit(X, Y, show_fig=True)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Using ANN class!\n",
    "X, Y = getBinaryData()\n",
    "\n",
    "# balance imbalance problem in the data\n",
    "X0 = X[Y==0, :]\n",
    "X1 = X[Y==1, :]\n",
    "# repeat class 1, 9 times\n",
    "X1 = np.repeat(X1, 9, axis=0)\n",
    "# stack X and Y\n",
    "X = np.vstack([X0, X1])\n",
    "Y = np.array([0]*len(X0) + [1]*len(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 907.9031569265344 error: 0.491\n",
      "i: 20 cost: 674.8802333124743 error: 0.39\n",
      "i: 40 cost: 661.2187813384332 error: 0.374\n",
      "i: 60 cost: 660.5005201358624 error: 0.408\n",
      "i: 80 cost: 661.9028229698006 error: 0.403\n",
      "i: 100 cost: 660.0785223186659 error: 0.398\n",
      "i: 120 cost: 659.1962193307143 error: 0.397\n",
      "i: 140 cost: 657.9415350207827 error: 0.395\n",
      "i: 160 cost: 656.143801648696 error: 0.39\n",
      "i: 180 cost: 654.2385352410699 error: 0.383\n",
      "i: 200 cost: 652.4658503916622 error: 0.382\n",
      "i: 220 cost: 650.8492752291222 error: 0.384\n",
      "i: 240 cost: 649.331409254781 error: 0.379\n",
      "i: 260 cost: 647.8626775032702 error: 0.38\n",
      "i: 280 cost: 646.4226262037051 error: 0.376\n",
      "i: 300 cost: 645.0036242768244 error: 0.379\n",
      "i: 320 cost: 643.6014100601981 error: 0.379\n",
      "i: 340 cost: 642.2134263708137 error: 0.377\n",
      "i: 360 cost: 640.8376247089001 error: 0.376\n",
      "i: 380 cost: 639.4716255871501 error: 0.375\n",
      "i: 400 cost: 638.1128991194603 error: 0.374\n",
      "i: 420 cost: 636.7591816241797 error: 0.374\n",
      "i: 440 cost: 635.4086215267059 error: 0.372\n",
      "i: 460 cost: 634.0597880535811 error: 0.376\n",
      "i: 480 cost: 632.7116732820423 error: 0.374\n",
      "i: 500 cost: 631.3636750302179 error: 0.371\n",
      "i: 520 cost: 630.0155532212863 error: 0.366\n",
      "i: 540 cost: 628.66737232675 error: 0.365\n",
      "i: 560 cost: 627.3194365401871 error: 0.365\n",
      "i: 580 cost: 625.9722244964471 error: 0.365\n",
      "i: 600 cost: 624.6263308846435 error: 0.36\n",
      "i: 620 cost: 623.2824184829299 error: 0.36\n",
      "i: 640 cost: 621.9411813215311 error: 0.36\n",
      "i: 660 cost: 620.6033181489154 error: 0.36\n",
      "i: 680 cost: 619.269514266974 error: 0.359\n",
      "i: 700 cost: 617.940429399009 error: 0.358\n",
      "i: 720 cost: 616.6166894412448 error: 0.355\n",
      "i: 740 cost: 615.29888048918 error: 0.351\n",
      "i: 760 cost: 613.9875442206064 error: 0.352\n",
      "i: 780 cost: 612.6831743033943 error: 0.351\n",
      "i: 800 cost: 611.3862138228372 error: 0.348\n",
      "i: 820 cost: 610.0970537819828 error: 0.346\n",
      "i: 840 cost: 608.8160326133291 error: 0.345\n",
      "i: 860 cost: 607.5434364815048 error: 0.343\n",
      "i: 880 cost: 606.2795000538605 error: 0.341\n",
      "i: 900 cost: 605.024407411407 error: 0.339\n",
      "i: 920 cost: 603.7782928573577 error: 0.338\n",
      "i: 940 cost: 602.5412415168587 error: 0.337\n",
      "i: 960 cost: 601.3132897651133 error: 0.335\n",
      "i: 980 cost: 600.0944256359132 error: 0.332\n",
      "i: 1000 cost: 598.8845894265459 error: 0.332\n",
      "i: 1020 cost: 597.6836747200837 error: 0.333\n",
      "i: 1040 cost: 596.491529994817 error: 0.331\n",
      "i: 1060 cost: 595.3079608921666 error: 0.328\n",
      "i: 1080 cost: 594.1327330813099 error: 0.326\n",
      "i: 1100 cost: 592.9655755055237 error: 0.321\n",
      "i: 1120 cost: 591.8061836406289 error: 0.319\n",
      "i: 1140 cost: 590.6542222679674 error: 0.32\n",
      "i: 1160 cost: 589.5093272071992 error: 0.319\n",
      "i: 1180 cost: 588.3711055311343 error: 0.319\n",
      "i: 1200 cost: 587.2391340672264 error: 0.319\n",
      "i: 1220 cost: 586.1129565211949 error: 0.318\n",
      "i: 1240 cost: 584.9920802861402 error: 0.316\n",
      "i: 1260 cost: 583.8759747120868 error: 0.316\n",
      "i: 1280 cost: 582.7640729290272 error: 0.311\n",
      "i: 1300 cost: 581.6557788462302 error: 0.311\n",
      "i: 1320 cost: 580.5504795676679 error: 0.309\n",
      "i: 1340 cost: 579.4475615673476 error: 0.308\n",
      "i: 1360 cost: 578.3464274157631 error: 0.309\n",
      "i: 1380 cost: 577.246509472359 error: 0.309\n",
      "i: 1400 cost: 576.1472779859332 error: 0.309\n",
      "i: 1420 cost: 575.0482429210585 error: 0.309\n",
      "i: 1440 cost: 573.948950619096 error: 0.308\n",
      "i: 1460 cost: 572.8489774048406 error: 0.306\n",
      "i: 1480 cost: 571.747922320089 error: 0.304\n",
      "i: 1500 cost: 570.6454006067968 error: 0.304\n",
      "i: 1520 cost: 569.5410388062709 error: 0.304\n",
      "i: 1540 cost: 568.4344716992446 error: 0.302\n",
      "i: 1560 cost: 567.3253409080252 error: 0.302\n",
      "i: 1580 cost: 566.2132948016335 error: 0.303\n",
      "i: 1600 cost: 565.0979893157374 error: 0.304\n",
      "i: 1620 cost: 563.9790893471834 error: 0.305\n",
      "i: 1640 cost: 562.8562704555054 error: 0.303\n",
      "i: 1660 cost: 561.7292206718803 error: 0.303\n",
      "i: 1680 cost: 560.5976422676601 error: 0.302\n",
      "i: 1700 cost: 559.4612533681793 error: 0.302\n",
      "i: 1720 cost: 558.3197893165626 error: 0.302\n",
      "i: 1740 cost: 557.1730037027949 error: 0.3\n",
      "i: 1760 cost: 556.0206689821425 error: 0.297\n",
      "i: 1780 cost: 554.8625766206424 error: 0.296\n",
      "i: 1800 cost: 553.6985367286924 error: 0.296\n",
      "i: 1820 cost: 552.5283771791803 error: 0.296\n",
      "i: 1840 cost: 551.3519422530421 error: 0.293\n",
      "i: 1860 cost: 550.1690909082415 error: 0.293\n",
      "i: 1880 cost: 548.9796948208455 error: 0.291\n",
      "i: 1900 cost: 547.7836363907822 error: 0.291\n",
      "i: 1920 cost: 546.5808069322941 error: 0.291\n",
      "i: 1940 cost: 545.3711052742176 error: 0.291\n",
      "i: 1960 cost: 544.1544369754026 error: 0.29\n",
      "i: 1980 cost: 542.930714316345 error: 0.289\n",
      "i: 2000 cost: 541.6998571630784 error: 0.287\n",
      "i: 2020 cost: 540.4617947194745 error: 0.284\n",
      "i: 2040 cost: 539.2164680968895 error: 0.284\n",
      "i: 2060 cost: 537.9638335440287 error: 0.281\n",
      "i: 2080 cost: 536.7038661036624 error: 0.281\n",
      "i: 2100 cost: 535.4365634046213 error: 0.279\n",
      "i: 2120 cost: 534.1619492642326 error: 0.279\n",
      "i: 2140 cost: 532.8800767727656 error: 0.28\n",
      "i: 2160 cost: 531.5910305591768 error: 0.281\n",
      "i: 2180 cost: 530.2949279945378 error: 0.281\n",
      "i: 2200 cost: 528.9919191703178 error: 0.28\n",
      "i: 2220 cost: 527.6821855844082 error: 0.28\n",
      "i: 2240 cost: 526.3659375677207 error: 0.279\n",
      "i: 2260 cost: 525.0434105774352 error: 0.278\n",
      "i: 2280 cost: 523.7148605598761 error: 0.276\n",
      "i: 2300 cost: 522.3805586398162 error: 0.276\n",
      "i: 2320 cost: 521.0407854204229 error: 0.276\n",
      "i: 2340 cost: 519.6958251794957 error: 0.276\n",
      "i: 2360 cost: 518.3459602265484 error: 0.276\n",
      "i: 2380 cost: 516.9914656472777 error: 0.275\n",
      "i: 2400 cost: 515.6326046135262 error: 0.274\n",
      "i: 2420 cost: 514.2696243844377 error: 0.274\n",
      "i: 2440 cost: 512.9027530735333 error: 0.274\n",
      "i: 2460 cost: 511.5321972109275 error: 0.274\n",
      "i: 2480 cost: 510.1581400922662 error: 0.272\n",
      "i: 2500 cost: 508.78074087716544 error: 0.272\n",
      "i: 2520 cost: 507.4001343797148 error: 0.271\n",
      "i: 2540 cost: 506.0164314809547 error: 0.271\n",
      "i: 2560 cost: 504.62972008664417 error: 0.27\n",
      "i: 2580 cost: 503.2400665514913 error: 0.268\n",
      "i: 2600 cost: 501.84751749179986 error: 0.267\n",
      "i: 2620 cost: 500.4521019109599 error: 0.266\n",
      "i: 2640 cost: 499.0538335654759 error: 0.265\n",
      "i: 2660 cost: 497.6527135027892 error: 0.265\n",
      "i: 2680 cost: 496.24873270585107 error: 0.265\n",
      "i: 2700 cost: 494.8418747833566 error: 0.265\n",
      "i: 2720 cost: 493.4321186490921 error: 0.264\n",
      "i: 2740 cost: 492.0194411392997 error: 0.264\n",
      "i: 2760 cost: 490.60381952363593 error: 0.264\n",
      "i: 2780 cost: 489.18523387324456 error: 0.264\n",
      "i: 2800 cost: 487.76366925847753 error: 0.262\n",
      "i: 2820 cost: 486.3391177583286 error: 0.259\n",
      "i: 2840 cost: 484.91158027286104 error: 0.258\n",
      "i: 2860 cost: 483.48106813781334 error: 0.258\n",
      "i: 2880 cost: 482.047604546104 error: 0.257\n",
      "i: 2900 cost: 480.6112257833315 error: 0.255\n",
      "i: 2920 cost: 479.1719822831079 error: 0.253\n",
      "i: 2940 cost: 477.7299395032934 error: 0.252\n",
      "i: 2960 cost: 476.28517861660947 error: 0.251\n",
      "i: 2980 cost: 474.8377969999842 error: 0.247\n",
      "i: 3000 cost: 473.3879084980127 error: 0.244\n",
      "i: 3020 cost: 471.9356434289382 error: 0.243\n",
      "i: 3040 cost: 470.48114829835043 error: 0.243\n",
      "i: 3060 cost: 469.02458518758476 error: 0.242\n",
      "i: 3080 cost: 467.5661307912427 error: 0.242\n",
      "i: 3100 cost: 466.1059750910332 error: 0.239\n",
      "i: 3120 cost: 464.644319670177 error: 0.236\n",
      "i: 3140 cost: 463.1813756920443 error: 0.235\n",
      "i: 3160 cost: 461.7173615862532 error: 0.233\n",
      "i: 3180 cost: 460.25250050278305 error: 0.233\n",
      "i: 3200 cost: 458.7870176077114 error: 0.232\n",
      "i: 3220 cost: 457.3211373016451 error: 0.232\n",
      "i: 3240 cost: 455.8550804432577 error: 0.231\n",
      "i: 3260 cost: 454.3890616561061 error: 0.231\n",
      "i: 3280 cost: 452.9232867883205 error: 0.23\n",
      "i: 3300 cost: 451.45795058378064 error: 0.229\n",
      "i: 3320 cost: 449.9932346120711 error: 0.228\n",
      "i: 3340 cost: 448.52930549479225 error: 0.226\n",
      "i: 3360 cost: 447.06631345912524 error: 0.224\n",
      "i: 3380 cost: 445.60439124651236 error: 0.221\n",
      "i: 3400 cost: 444.14365340468726 error: 0.221\n",
      "i: 3420 cost: 442.68419599384004 error: 0.219\n",
      "i: 3440 cost: 441.2260967405676 error: 0.218\n",
      "i: 3460 cost: 439.76941567398455 error: 0.217\n",
      "i: 3480 cost: 438.314196274556 error: 0.216\n",
      "i: 3500 cost: 436.8604671556936 error: 0.214\n",
      "i: 3520 cost: 435.40824427966953 error: 0.212\n",
      "i: 3540 cost: 433.95753368279634 error: 0.213\n",
      "i: 3560 cost: 432.50833465138805 error: 0.212\n",
      "i: 3580 cost: 431.06064325254573 error: 0.212\n",
      "i: 3600 cost: 429.61445608632187 error: 0.212\n",
      "i: 3620 cost: 428.1697740931976 error: 0.21\n",
      "i: 3640 cost: 426.7266062280525 error: 0.209\n",
      "i: 3660 cost: 425.2849728032754 error: 0.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 3680 cost: 423.844908312184 error: 0.205\n",
      "i: 3700 cost: 422.4064635702618 error: 0.205\n",
      "i: 3720 cost: 420.9697070541321 error: 0.204\n",
      "i: 3740 cost: 419.53472537265054 error: 0.203\n",
      "i: 3760 cost: 418.101622865153 error: 0.202\n",
      "i: 3780 cost: 416.670520382023 error: 0.201\n",
      "i: 3800 cost: 415.2415533557944 error: 0.199\n",
      "i: 3820 cost: 413.8148693116872 error: 0.197\n",
      "i: 3840 cost: 412.390624991349 error: 0.197\n",
      "i: 3860 cost: 410.9689832716482 error: 0.194\n",
      "i: 3880 cost: 409.55011005267966 error: 0.193\n",
      "i: 3900 cost: 408.1341712686259 error: 0.192\n",
      "i: 3920 cost: 406.7213301456079 error: 0.192\n",
      "i: 3940 cost: 405.3117447964328 error: 0.189\n",
      "i: 3960 cost: 403.90556620711595 error: 0.189\n",
      "i: 3980 cost: 402.5029366374169 error: 0.187\n",
      "i: 4000 cost: 401.10398842966913 error: 0.187\n",
      "i: 4020 cost: 399.70884319816406 error: 0.185\n",
      "i: 4040 cost: 398.317611355772 error: 0.183\n",
      "i: 4060 cost: 396.93039192509707 error: 0.182\n",
      "i: 4080 cost: 395.5472725776706 error: 0.182\n",
      "i: 4100 cost: 394.1683298455831 error: 0.181\n",
      "i: 4120 cost: 392.7936294545243 error: 0.18\n",
      "i: 4140 cost: 391.4232267344345 error: 0.18\n",
      "i: 4160 cost: 390.0571670728947 error: 0.177\n",
      "i: 4180 cost: 388.69548638605676 error: 0.177\n",
      "i: 4200 cost: 387.3382115914613 error: 0.177\n",
      "i: 4220 cost: 385.9853610757736 error: 0.175\n",
      "i: 4240 cost: 384.63694515752684 error: 0.175\n",
      "i: 4260 cost: 383.2929665498702 error: 0.175\n",
      "i: 4280 cost: 381.953420830709 error: 0.175\n",
      "i: 4300 cost: 380.6182969273598 error: 0.175\n",
      "i: 4320 cost: 379.287577620153 error: 0.175\n",
      "i: 4340 cost: 377.96124006474474 error: 0.174\n",
      "i: 4360 cost: 376.63925632714285 error: 0.174\n",
      "i: 4380 cost: 375.32159391951143 error: 0.17\n",
      "i: 4400 cost: 374.0082163197798 error: 0.169\n",
      "i: 4420 cost: 372.69908345484777 error: 0.169\n",
      "i: 4440 cost: 371.3941521263831 error: 0.171\n",
      "i: 4460 cost: 370.0933763600536 error: 0.171\n",
      "i: 4480 cost: 368.7967076633731 error: 0.17\n",
      "i: 4500 cost: 367.50409518355787 error: 0.17\n",
      "i: 4520 cost: 366.2154857640917 error: 0.169\n",
      "i: 4540 cost: 364.930823906153 error: 0.169\n",
      "i: 4560 cost: 363.6500516478426 error: 0.169\n",
      "i: 4580 cost: 362.37310837960416 error: 0.168\n",
      "i: 4600 cost: 361.0999306179935 error: 0.168\n",
      "i: 4620 cost: 359.83045176188205 error: 0.168\n",
      "i: 4640 cost: 358.5646018554321 error: 0.165\n",
      "i: 4660 cost: 357.30230738097976 error: 0.165\n",
      "i: 4680 cost: 356.0434911026795 error: 0.164\n",
      "i: 4700 cost: 354.7880719787513 error: 0.163\n",
      "i: 4720 cost: 353.5359651566886 error: 0.162\n",
      "i: 4740 cost: 352.28708206214117 error: 0.161\n",
      "i: 4760 cost: 351.0413305884563 error: 0.16\n",
      "i: 4780 cost: 349.79861539021766 error: 0.16\n",
      "i: 4800 cost: 348.55883828059854 error: 0.159\n",
      "i: 4820 cost: 347.32189872894963 error: 0.159\n",
      "i: 4840 cost: 346.0876944518629 error: 0.158\n",
      "i: 4860 cost: 344.8561220879925 error: 0.157\n",
      "i: 4880 cost: 343.62707794421954 error: 0.157\n",
      "i: 4900 cost: 342.4004587984059 error: 0.157\n",
      "i: 4920 cost: 341.17616274203397 error: 0.157\n",
      "i: 4940 cost: 339.95409004453757 error: 0.157\n",
      "i: 4960 cost: 338.7341440201548 error: 0.157\n",
      "i: 4980 cost: 337.51623187769104 error: 0.156\n",
      "i: 5000 cost: 336.30026553372306 error: 0.156\n",
      "i: 5020 cost: 335.086162370483 error: 0.154\n",
      "i: 5040 cost: 333.87384592096066 error: 0.153\n",
      "i: 5060 cost: 332.6632464656278 error: 0.153\n",
      "i: 5080 cost: 331.4543015276398 error: 0.153\n",
      "i: 5100 cost: 330.24695625634916 error: 0.152\n",
      "i: 5120 cost: 329.04116369243303 error: 0.15\n",
      "i: 5140 cost: 327.83688491180214 error: 0.15\n",
      "i: 5160 cost: 326.63408904960795 error: 0.149\n",
      "i: 5180 cost: 325.43275320983525 error: 0.149\n",
      "i: 5200 cost: 324.2328622700371 error: 0.148\n",
      "i: 5220 cost: 323.0344085943648 error: 0.148\n",
      "i: 5240 cost: 321.8373916710185 error: 0.146\n",
      "i: 5260 cost: 320.6418176922973 error: 0.146\n",
      "i: 5280 cost: 319.4476990964498 error: 0.146\n",
      "i: 5300 cost: 318.2550540904616 error: 0.145\n",
      "i: 5320 cost: 317.0639061717743 error: 0.145\n",
      "i: 5340 cost: 315.8742836648657 error: 0.145\n",
      "i: 5360 cost: 314.6862192858283 error: 0.145\n",
      "i: 5380 cost: 313.4997497448292 error: 0.145\n",
      "i: 5400 cost: 312.3149153928779 error: 0.145\n",
      "i: 5420 cost: 311.13175991593164 error: 0.144\n",
      "i: 5440 cost: 309.9503300762953 error: 0.143\n",
      "i: 5460 cost: 308.77067549860044 error: 0.142\n",
      "i: 5480 cost: 307.59284849560373 error: 0.142\n",
      "i: 5500 cost: 306.41690392756755 error: 0.142\n",
      "i: 5520 cost: 305.24289908814336 error: 0.142\n",
      "i: 5540 cost: 304.07089360936936 error: 0.142\n",
      "i: 5560 cost: 302.9009493785361 error: 0.142\n",
      "i: 5580 cost: 301.73313046018785 error: 0.142\n",
      "i: 5600 cost: 300.56750301726413 error: 0.142\n",
      "i: 5620 cost: 299.40413522624146 error: 0.142\n",
      "i: 5640 cost: 298.24309718204313 error: 0.141\n",
      "i: 5660 cost: 297.0844607893008 error: 0.14\n",
      "i: 5680 cost: 295.9282996373008 error: 0.139\n",
      "i: 5700 cost: 294.77468885652206 error: 0.139\n",
      "i: 5720 cost: 293.6237049551293 error: 0.138\n",
      "i: 5740 cost: 292.4754256341022 error: 0.138\n",
      "i: 5760 cost: 291.3299295799657 error: 0.137\n",
      "i: 5780 cost: 290.18729623426873 error: 0.136\n",
      "i: 5800 cost: 289.0476055392121 error: 0.136\n",
      "i: 5820 cost: 287.9109376591914 error: 0.135\n",
      "i: 5840 cost: 286.7773726785061 error: 0.135\n",
      "i: 5860 cost: 285.646990276142 error: 0.133\n",
      "i: 5880 cost: 284.5198693792837 error: 0.132\n",
      "i: 5900 cost: 283.39608779828075 error: 0.131\n",
      "i: 5920 cost: 282.2757218469318 error: 0.131\n",
      "i: 5940 cost: 281.1588459529878 error: 0.131\n",
      "i: 5960 cost: 280.04553226482193 error: 0.13\n",
      "i: 5980 cost: 278.93585026143637 error: 0.129\n",
      "i: 6000 cost: 277.82986637369805 error: 0.129\n",
      "i: 6020 cost: 276.7276436247529 error: 0.129\n",
      "i: 6040 cost: 275.6292412980451 error: 0.128\n",
      "i: 6060 cost: 274.534714641548 error: 0.128\n",
      "i: 6080 cost: 273.44411461505047 error: 0.128\n",
      "i: 6100 cost: 272.3574876864126 error: 0.127\n",
      "i: 6120 cost: 271.2748756834661 error: 0.126\n",
      "i: 6140 cost: 270.19631570407347 error: 0.126\n",
      "i: 6160 cost: 269.1218400838659 error: 0.126\n",
      "i: 6180 cost: 268.0514764265764 error: 0.124\n",
      "i: 6200 cost: 266.9852476932044 error: 0.123\n",
      "i: 6220 cost: 265.9231723388445 error: 0.123\n",
      "i: 6240 cost: 264.865264509547 error: 0.121\n",
      "i: 6260 cost: 263.8115342835382 error: 0.119\n",
      "i: 6280 cost: 262.76198792716775 error: 0.118\n",
      "i: 6300 cost: 261.7166282241854 error: 0.116\n",
      "i: 6320 cost: 260.67545480265863 error: 0.116\n",
      "i: 6340 cost: 259.6384644218292 error: 0.116\n",
      "i: 6360 cost: 258.6056514496989 error: 0.116\n",
      "i: 6380 cost: 257.57700809313525 error: 0.116\n",
      "i: 6400 cost: 256.55252469643443 error: 0.116\n",
      "i: 6420 cost: 255.53219053975587 error: 0.116\n",
      "i: 6440 cost: 254.51599319147243 error: 0.116\n",
      "i: 6460 cost: 253.5039199431151 error: 0.116\n",
      "i: 6480 cost: 252.49595799340082 error: 0.115\n",
      "i: 6500 cost: 251.4920916257263 error: 0.115\n",
      "i: 6520 cost: 250.49231113003498 error: 0.114\n",
      "i: 6540 cost: 249.4965981376143 error: 0.112\n",
      "i: 6560 cost: 248.504941246624 error: 0.111\n",
      "i: 6580 cost: 247.5173381449918 error: 0.111\n",
      "i: 6600 cost: 246.53373961850986 error: 0.111\n",
      "i: 6620 cost: 245.5542155701615 error: 0.111\n",
      "i: 6640 cost: 244.57862276438613 error: 0.11\n",
      "i: 6660 cost: 243.60710217319416 error: 0.109\n",
      "i: 6680 cost: 242.63960977002915 error: 0.108\n",
      "i: 6700 cost: 241.6757212478297 error: 0.108\n",
      "i: 6720 cost: 240.7169766566538 error: 0.107\n",
      "i: 6740 cost: 239.75951627460196 error: 0.106\n",
      "i: 6760 cost: 238.81118931537097 error: 0.105\n",
      "i: 6780 cost: 237.85835461182353 error: 0.105\n",
      "i: 6800 cost: 236.9205820395947 error: 0.105\n",
      "i: 6820 cost: 235.97849737967772 error: 0.105\n",
      "i: 6840 cost: 235.0276950813738 error: 0.105\n",
      "i: 6860 cost: 234.16161373745393 error: 0.104\n",
      "i: 6880 cost: 233.04090383128056 error: 0.103\n",
      "i: 6900 cost: 232.59803477448617 error: 0.103\n",
      "i: 6920 cost: 230.58907812261089 error: 0.102\n",
      "i: 6940 cost: 232.01527535648756 error: 0.102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fb15ab2875e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fit the data into the model, use tanh activation. ** note: after testing few times, relu activation function doesn't behave well in this model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_fig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4eb17b51d7de>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, learning_rate, reg, epochs, show_fig)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# dZ = np.outer(pY_Y, self.W2) * (Z > 0) # ReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update input-to-hidden weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# update input-to-hidden bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = ANN(100) # hidden layer size to 100\n",
    "\n",
    "# fit the data into the model, use tanh activation. ** note: after testing few times, relu activation function doesn't behave well in this model.\n",
    "model.fit(X , Y, show_fig=True, epochs = 10000)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Multiple Classfication with Softmax\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNwithSoftmax(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=10e-6, reg=10e-1, epochs=10000, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        # Tvalid = y2indicator(Yvalid)\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y_to_indicator(Y)\n",
    "        \n",
    "        # input-to-hidden layer\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        \n",
    "        # hidden-to-output layer\n",
    "        self.W2 = np.random.randn(self.M, K) / np.sqrt(self.M)\n",
    "        self.b2 = np.zeros(K)\n",
    "\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation and cost calculation\n",
    "            pY, Z = self.forward(X)\n",
    "\n",
    "            # gradient descent step\n",
    "            pY_T = pY - T\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_T) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*(pY_T.sum(axis=0) + reg*self.b2)\n",
    "            \n",
    "            # dZ = pY_T.dot(self.W2.T) * (Z > 0) # relu\n",
    "            dZ = pY_T.dot(self.W2.T) * (1 - Z*Z) # tanh\n",
    "            \n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate*(dZ.sum(axis=0) + reg*self.b1)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = cost2(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(pYvalid, axis=1))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        print(\"validation score: \", self.score(Xvalid, Yvalid))\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return softmax(Z.dot(self.W2) + self.b2), Z\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     X, Y = getData()\n",
    "    \n",
    "#     model = ANN(200)\n",
    "#     model.fit(X, Y, reg=0, show_fig=True)\n",
    "#     print(model.score(X, Y))\n",
    "#     # scores = cross_val_score(model, X, Y, cv=5)\n",
    "#     # print \"score mean:\", np.mean(scores), \"stdev:\", np.std(scores)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 4.176066641187003 error: 0.516\n",
      "i: 20 cost: 4.799642246762398 error: 0.516\n",
      "i: 40 cost: 5.303824425901825 error: 0.516\n",
      "i: 60 cost: 5.79741145915397 error: 0.516\n",
      "i: 80 cost: 6.11922547959057 error: 0.516\n",
      "i: 100 cost: 6.258828662922845 error: 0.516\n",
      "i: 120 cost: 6.314284146357235 error: 0.516\n",
      "i: 140 cost: 6.342854805581711 error: 0.516\n",
      "i: 160 cost: 6.361919052360248 error: 0.516\n",
      "i: 180 cost: 6.376343304131984 error: 0.516\n",
      "i: 200 cost: 6.387873316895359 error: 0.516\n",
      "i: 220 cost: 6.39737243359595 error: 0.516\n",
      "i: 240 cost: 6.4053636280265795 error: 0.516\n",
      "i: 260 cost: 6.412196182693226 error: 0.516\n",
      "i: 280 cost: 6.418115802238994 error: 0.516\n",
      "i: 300 cost: 6.423301404391709 error: 0.516\n",
      "i: 320 cost: 6.427886792898999 error: 0.516\n",
      "i: 340 cost: 6.431974254290928 error: 0.516\n",
      "i: 360 cost: 6.4356434516825765 error: 0.516\n",
      "i: 380 cost: 6.438957433030001 error: 0.516\n",
      "i: 400 cost: 6.441966800332941 error: 0.516\n",
      "i: 420 cost: 6.4447126700561155 error: 0.516\n",
      "i: 440 cost: 6.447228817914773 error: 0.516\n",
      "i: 460 cost: 6.449543260589566 error: 0.516\n",
      "i: 480 cost: 6.4516794408268305 error: 0.516\n",
      "best_validation_error: 0.516\n",
      "validation score:  0.518\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4992912110166059\n"
     ]
    }
   ],
   "source": [
    "model = ANNwithSoftmax(200)\n",
    "model.fit(X, Y, reg=0, show_fig=True)\n",
    "print(model.score(X, Y))\n",
    "# scores = cross_val_score(model, X, Y, cv=5)\n",
    "# print \"score mean:\", np.mean(scores), \"stdev:\", np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W, b = init_weight_and_bias(M1, M2)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "class ANN_TF(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=1e-2, mu=0.99, decay=0.999, reg=1e-3, epochs=10, batch_sz=100, show_fig=False):\n",
    "        K = len(set(Y)) # won't work later b/c we turn it into indicator\n",
    "\n",
    "        # make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = y_to_indicator(Y).astype(np.float32)\n",
    "        # Y = Y.astype(np.int32)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        Yvalid_flat = np.argmax(Yvalid, axis=1) # for calculating error rate\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        # initialize hidden layers\n",
    "        N, D = X.shape\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        M1 = D\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # set up theano functions and variables\n",
    "        tfX = tf.placeholder(tf.float32, shape=(None, D), name='X')\n",
    "        tfT = tf.placeholder(tf.float32, shape=(None, K), name='T')\n",
    "        act = self.forward(tfX)\n",
    "\n",
    "        rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=act,\n",
    "                labels=tfT\n",
    "            )\n",
    "        ) + rcost\n",
    "        prediction = self.predict(tfX)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu).minimize(cost)\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            for i in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                for j in range(n_batches):\n",
    "                    Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                    Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                    session.run(train_op, feed_dict={tfX: Xbatch, tfT: Ybatch})\n",
    "\n",
    "                    if j % 20 == 0:\n",
    "                        c = session.run(cost, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        costs.append(c)\n",
    "\n",
    "                        p = session.run(prediction, feed_dict={tfX: Xvalid, tfT: Yvalid})\n",
    "                        e = error_rate(Yvalid_flat, p)\n",
    "                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        act = self.forward(X)\n",
    "        return tf.argmax(act, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-ca250fd955ad>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "i: 0 j: 0 nb: 392 cost: 2.8192596 error rate: 0.849\n",
      "i: 0 j: 20 nb: 392 cost: 2.766212 error rate: 0.77\n",
      "i: 0 j: 40 nb: 392 cost: 2.7515898 error rate: 0.733\n",
      "i: 0 j: 60 nb: 392 cost: 2.7332249 error rate: 0.751\n",
      "i: 0 j: 80 nb: 392 cost: 2.7334073 error rate: 0.773\n",
      "i: 0 j: 100 nb: 392 cost: 2.6375074 error rate: 0.699\n",
      "i: 0 j: 120 nb: 392 cost: 2.7434754 error rate: 0.705\n",
      "i: 0 j: 140 nb: 392 cost: 2.606319 error rate: 0.74\n",
      "i: 0 j: 160 nb: 392 cost: 2.5954235 error rate: 0.722\n",
      "i: 0 j: 180 nb: 392 cost: 2.5480108 error rate: 0.694\n",
      "i: 0 j: 200 nb: 392 cost: 2.6018639 error rate: 0.751\n",
      "i: 0 j: 220 nb: 392 cost: 2.5449119 error rate: 0.776\n",
      "i: 0 j: 240 nb: 392 cost: 2.5479531 error rate: 0.731\n",
      "i: 0 j: 260 nb: 392 cost: 2.5007927 error rate: 0.7\n",
      "i: 0 j: 280 nb: 392 cost: 2.498108 error rate: 0.705\n",
      "i: 0 j: 300 nb: 392 cost: 2.546213 error rate: 0.805\n",
      "i: 0 j: 320 nb: 392 cost: 2.4489875 error rate: 0.735\n",
      "i: 0 j: 340 nb: 392 cost: 2.4234757 error rate: 0.72\n",
      "i: 0 j: 360 nb: 392 cost: 2.374714 error rate: 0.717\n",
      "i: 0 j: 380 nb: 392 cost: 2.3501205 error rate: 0.697\n",
      "i: 1 j: 0 nb: 392 cost: 2.4099727 error rate: 0.749\n",
      "i: 1 j: 20 nb: 392 cost: 2.3970556 error rate: 0.802\n",
      "i: 1 j: 40 nb: 392 cost: 2.3046167 error rate: 0.696\n",
      "i: 1 j: 60 nb: 392 cost: 2.2710793 error rate: 0.688\n",
      "i: 1 j: 80 nb: 392 cost: 2.3992186 error rate: 0.787\n",
      "i: 1 j: 100 nb: 392 cost: 2.3076584 error rate: 0.735\n",
      "i: 1 j: 120 nb: 392 cost: 2.290923 error rate: 0.728\n",
      "i: 1 j: 140 nb: 392 cost: 2.297583 error rate: 0.746\n",
      "i: 1 j: 160 nb: 392 cost: 2.253757 error rate: 0.72\n",
      "i: 1 j: 180 nb: 392 cost: 2.2548943 error rate: 0.724\n",
      "i: 1 j: 200 nb: 392 cost: 2.2382154 error rate: 0.749\n",
      "i: 1 j: 220 nb: 392 cost: 2.2324884 error rate: 0.74\n",
      "i: 1 j: 240 nb: 392 cost: 2.2088702 error rate: 0.74\n",
      "i: 1 j: 260 nb: 392 cost: 2.1619875 error rate: 0.704\n",
      "i: 1 j: 280 nb: 392 cost: 2.2459671 error rate: 0.751\n",
      "i: 1 j: 300 nb: 392 cost: 2.2612314 error rate: 0.756\n",
      "i: 1 j: 320 nb: 392 cost: 2.1853974 error rate: 0.738\n",
      "i: 1 j: 340 nb: 392 cost: 2.179161 error rate: 0.741\n",
      "i: 1 j: 360 nb: 392 cost: 2.1755333 error rate: 0.728\n",
      "i: 1 j: 380 nb: 392 cost: 2.157445 error rate: 0.759\n",
      "i: 2 j: 0 nb: 392 cost: 2.1280956 error rate: 0.711\n",
      "i: 2 j: 20 nb: 392 cost: 2.1331198 error rate: 0.72\n",
      "i: 2 j: 40 nb: 392 cost: 2.1244516 error rate: 0.724\n",
      "i: 2 j: 60 nb: 392 cost: 2.0824795 error rate: 0.715\n",
      "i: 2 j: 80 nb: 392 cost: 2.09277 error rate: 0.725\n",
      "i: 2 j: 100 nb: 392 cost: 2.0773034 error rate: 0.706\n",
      "i: 2 j: 120 nb: 392 cost: 2.0681431 error rate: 0.713\n",
      "i: 2 j: 140 nb: 392 cost: 2.098616 error rate: 0.798\n",
      "i: 2 j: 160 nb: 392 cost: 2.0373545 error rate: 0.709\n",
      "i: 2 j: 180 nb: 392 cost: 2.049365 error rate: 0.728\n",
      "i: 2 j: 200 nb: 392 cost: 2.0736315 error rate: 0.772\n",
      "i: 2 j: 220 nb: 392 cost: 2.0586033 error rate: 0.745\n",
      "i: 2 j: 240 nb: 392 cost: 2.033162 error rate: 0.73\n",
      "i: 2 j: 260 nb: 392 cost: 2.0209668 error rate: 0.748\n",
      "i: 2 j: 280 nb: 392 cost: 2.0356727 error rate: 0.756\n",
      "i: 2 j: 300 nb: 392 cost: 1.9995807 error rate: 0.735\n",
      "i: 2 j: 320 nb: 392 cost: 1.9937228 error rate: 0.738\n",
      "i: 2 j: 340 nb: 392 cost: 1.9914439 error rate: 0.744\n",
      "i: 2 j: 360 nb: 392 cost: 2.0762177 error rate: 0.799\n",
      "i: 2 j: 380 nb: 392 cost: 1.9667596 error rate: 0.734\n",
      "i: 3 j: 0 nb: 392 cost: 1.9516441 error rate: 0.74\n",
      "i: 3 j: 20 nb: 392 cost: 1.9461493 error rate: 0.743\n",
      "i: 3 j: 40 nb: 392 cost: 1.9762468 error rate: 0.732\n",
      "i: 3 j: 60 nb: 392 cost: 1.9347847 error rate: 0.721\n",
      "i: 3 j: 80 nb: 392 cost: 1.9670383 error rate: 0.749\n",
      "i: 3 j: 100 nb: 392 cost: 1.9346985 error rate: 0.741\n",
      "i: 3 j: 120 nb: 392 cost: 2.0042496 error rate: 0.753\n",
      "i: 3 j: 140 nb: 392 cost: 1.9978075 error rate: 0.737\n",
      "i: 3 j: 160 nb: 392 cost: 1.9476192 error rate: 0.774\n",
      "i: 3 j: 180 nb: 392 cost: 1.9644257 error rate: 0.735\n",
      "i: 3 j: 200 nb: 392 cost: 1.9672544 error rate: 0.762\n",
      "i: 3 j: 220 nb: 392 cost: 1.9955859 error rate: 0.864\n",
      "i: 3 j: 240 nb: 392 cost: 1.9668657 error rate: 0.77\n",
      "i: 3 j: 260 nb: 392 cost: 1.9496266 error rate: 0.78\n",
      "i: 3 j: 280 nb: 392 cost: 1.9612868 error rate: 0.765\n",
      "i: 3 j: 300 nb: 392 cost: 1.9610982 error rate: 0.761\n",
      "i: 3 j: 320 nb: 392 cost: 1.9328661 error rate: 0.746\n",
      "i: 3 j: 340 nb: 392 cost: 1.9351506 error rate: 0.753\n",
      "i: 3 j: 360 nb: 392 cost: 1.9357923 error rate: 0.77\n",
      "i: 3 j: 380 nb: 392 cost: 1.950578 error rate: 0.77\n",
      "i: 4 j: 0 nb: 392 cost: 1.9549791 error rate: 0.77\n",
      "i: 4 j: 20 nb: 392 cost: 1.9574732 error rate: 0.768\n",
      "i: 4 j: 40 nb: 392 cost: 1.9613082 error rate: 0.77\n",
      "i: 4 j: 60 nb: 392 cost: 1.9447249 error rate: 0.776\n",
      "i: 4 j: 80 nb: 392 cost: 1.9423499 error rate: 0.765\n",
      "i: 4 j: 100 nb: 392 cost: 1.9271508 error rate: 0.746\n",
      "i: 4 j: 120 nb: 392 cost: 1.956042 error rate: 0.77\n",
      "i: 4 j: 140 nb: 392 cost: 1.9726816 error rate: 0.797\n",
      "i: 4 j: 160 nb: 392 cost: 1.936665 error rate: 0.756\n",
      "i: 4 j: 180 nb: 392 cost: 1.9617409 error rate: 0.785\n",
      "i: 4 j: 200 nb: 392 cost: 2.0075035 error rate: 0.761\n",
      "i: 4 j: 220 nb: 392 cost: 1.944608 error rate: 0.749\n",
      "i: 4 j: 240 nb: 392 cost: 1.9336673 error rate: 0.753\n",
      "i: 4 j: 260 nb: 392 cost: 1.928409 error rate: 0.762\n",
      "i: 4 j: 280 nb: 392 cost: 1.9757242 error rate: 0.843\n",
      "i: 4 j: 300 nb: 392 cost: 1.9535279 error rate: 0.72\n",
      "i: 4 j: 320 nb: 392 cost: 1.9315343 error rate: 0.728\n",
      "i: 4 j: 340 nb: 392 cost: 1.9463949 error rate: 0.752\n",
      "i: 4 j: 360 nb: 392 cost: 1.9918535 error rate: 0.768\n",
      "i: 4 j: 380 nb: 392 cost: 1.9752345 error rate: 0.77\n",
      "i: 5 j: 0 nb: 392 cost: 1.9741526 error rate: 0.77\n",
      "i: 5 j: 20 nb: 392 cost: 1.9853185 error rate: 0.77\n",
      "i: 5 j: 40 nb: 392 cost: 1.9872274 error rate: 0.77\n",
      "i: 5 j: 60 nb: 392 cost: 1.9890878 error rate: 0.77\n",
      "i: 5 j: 80 nb: 392 cost: 1.9781156 error rate: 0.77\n",
      "i: 5 j: 100 nb: 392 cost: 1.9693364 error rate: 0.77\n",
      "i: 5 j: 120 nb: 392 cost: 1.9720069 error rate: 0.77\n",
      "i: 5 j: 140 nb: 392 cost: 1.9775188 error rate: 0.77\n",
      "i: 5 j: 160 nb: 392 cost: 1.9659847 error rate: 0.77\n",
      "i: 5 j: 180 nb: 392 cost: 1.9559675 error rate: 0.77\n",
      "i: 5 j: 200 nb: 392 cost: 1.9550059 error rate: 0.77\n",
      "i: 5 j: 220 nb: 392 cost: 1.9522226 error rate: 0.77\n",
      "i: 5 j: 240 nb: 392 cost: 1.9447547 error rate: 0.77\n",
      "i: 5 j: 260 nb: 392 cost: 1.9473063 error rate: 0.77\n",
      "i: 5 j: 280 nb: 392 cost: 1.9397916 error rate: 0.77\n",
      "i: 5 j: 300 nb: 392 cost: 1.9466528 error rate: 0.77\n",
      "i: 5 j: 320 nb: 392 cost: 1.9410589 error rate: 0.77\n",
      "i: 5 j: 340 nb: 392 cost: 1.9542415 error rate: 0.77\n",
      "i: 5 j: 360 nb: 392 cost: 1.9509192 error rate: 0.77\n",
      "i: 5 j: 380 nb: 392 cost: 1.9406334 error rate: 0.77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-133be90f6854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANN_TF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_fig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-ca250fd955ad>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, learning_rate, mu, decay, reg, epochs, batch_sz, show_fig)\u001b[0m\n\u001b[0;32m     74\u001b[0m                     \u001b[0mYbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtfX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mXbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mYbatch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run the data\n",
    "X, Y = getData()\n",
    "model = ANN_TF([2000, 1000])\n",
    "model.fit(X, Y, show_fig = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "    def __init__(self, mi, mo, fw=5, fh=5, poolsz=(2, 2)):\n",
    "        # mi = input feature map size\n",
    "        # mo = output feature map size\n",
    "        sz = (fw, fh, mi, mo)\n",
    "        W0 = init_filter(sz, poolsz)\n",
    "        self.W = tf.Variable(W0)\n",
    "        b0 = np.zeros(mo, dtype=np.float32)\n",
    "        self.b = tf.Variable(b0)\n",
    "        self.poolsz = poolsz\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        conv_out = tf.nn.conv2d(X, self.W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv_out = tf.nn.bias_add(conv_out, self.b)\n",
    "        p1, p2 = self.poolsz\n",
    "        pool_out = tf.nn.max_pool(\n",
    "            conv_out,\n",
    "            ksize=[1, p1, p2, 1],\n",
    "            strides=[1, p1, p2, 1],\n",
    "            padding='SAME'\n",
    "        )\n",
    "        return tf.tanh(pool_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        W, b = init_weight_and_bias(M1, M2)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.matmul(X, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_TF(object):\n",
    "    def __init__(self, convpool_layer_sizes, hidden_layer_sizes):\n",
    "        self.convpool_layer_sizes = convpool_layer_sizes\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        \n",
    "    def fit(self, X, Y, learning_rate=1e-2, mu=0.99, decay=0.999, reg=1e-3, epochs=10, batch_sz=100, show_fig=False):\n",
    "        lr = np.float32(learning_rate)\n",
    "        mu = np.float32(mu)\n",
    "        reg = np.float32(reg)\n",
    "        decay = np.float32(decay)\n",
    "        eps = np.float32(epochs)\n",
    "        K = len(set(Y))\n",
    "\n",
    "        # make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = y2indicator(Y).astype(np.float32)\n",
    "\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "        Yvalid_flat = np.argmax(Yvalid, axis=1) # for calculating error rate\n",
    "\n",
    "        # initialize convpool layers\n",
    "        N, width, height, c = X.shape\n",
    "        mi = c\n",
    "        outw = width\n",
    "        outh = height\n",
    "        self.convpool_layers = []\n",
    "        for mo, fw, fh in self.convpool_layer_sizes:\n",
    "            layer = ConvPoolLayer(mi, mo, fw, fh)\n",
    "            self.convpool_layers.append(layer)\n",
    "            outw = outw // 2\n",
    "            outh = outh // 2\n",
    "            mi = mo\n",
    "\n",
    "        # initialize mlp layers\n",
    "        self.hidden_layers = []\n",
    "        M1 = self.convpool_layer_sizes[-1][0]*outw*outh # size must be same as output of last convpool layer\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "\n",
    "        # logistic regression layer\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        self.W = tf.Variable(W, 'W_logreg')\n",
    "        self.b = tf.Variable(b, 'b_logreg')\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.convpool_layers:\n",
    "            self.params += h.params\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # set up tensorflow functions and variables\n",
    "        tfX = tf.placeholder(tf.float32, shape=(None, width, height, c), name='X')\n",
    "        tfY = tf.placeholder(tf.float32, shape=(None, K), name='Y')\n",
    "        act = self.forward(tfX)\n",
    "\n",
    "        rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=act,\n",
    "                labels=tfY\n",
    "            )\n",
    "        ) + rcost\n",
    "        prediction = self.predict(tfX)\n",
    "\n",
    "        train_op = tf.train.RMSPropOptimizer(lr, decay=decay, momentum=mu).minimize(cost)\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            for i in range(epochs):\n",
    "                X, Y = shuffle(X, Y)\n",
    "                for j in range(n_batches):\n",
    "                    Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                    Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                    session.run(train_op, feed_dict={tfX: Xbatch, tfY: Ybatch})\n",
    "\n",
    "                    if j % 20 == 0:\n",
    "                        c = session.run(cost, feed_dict={tfX: Xvalid, tfY: Yvalid})\n",
    "                        costs.append(c)\n",
    "\n",
    "                        p = session.run(prediction, feed_dict={tfX: Xvalid, tfY: Yvalid})\n",
    "                        e = error_rate(Yvalid_flat, p)\n",
    "                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for c in self.convpool_layers:\n",
    "            Z = c.forward(Z)\n",
    "        Z_shape = Z.get_shape().as_list()\n",
    "        Z = tf.reshape(Z, [-1, np.prod(Z_shape[1:])])\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return tf.matmul(Z, self.W) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY = self.forward(X)\n",
    "        return tf.argmax(pY, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(X):\n",
    "    arr = np.zeros(shape=(X.shape[0], 48, 48))\n",
    "    for i in range(X.shape[0]):\n",
    "        arr[i] = np.reshape(X[i], (48, 48))\n",
    "    \n",
    "    return arr[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-38dcc1d7a58f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model = CNN(\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mconvpool_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CNN' is not defined"
     ]
    }
   ],
   "source": [
    "X, Y = getData()\n",
    "\n",
    "X = rearrange(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-d10f97011a66>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "i: 0 j: 0 nb: 392 cost: 2.282291 error rate: 0.894\n",
      "i: 0 j: 20 nb: 392 cost: 2.2208674 error rate: 0.755\n",
      "i: 0 j: 40 nb: 392 cost: 2.2134507 error rate: 0.755\n",
      "i: 0 j: 60 nb: 392 cost: 2.1957757 error rate: 0.755\n",
      "i: 0 j: 80 nb: 392 cost: 2.165794 error rate: 0.731\n",
      "i: 0 j: 100 nb: 392 cost: 2.1235492 error rate: 0.717\n",
      "i: 0 j: 120 nb: 392 cost: 2.033295 error rate: 0.682\n",
      "i: 0 j: 140 nb: 392 cost: 1.9938495 error rate: 0.654\n",
      "i: 0 j: 160 nb: 392 cost: 2.0390906 error rate: 0.648\n",
      "i: 0 j: 180 nb: 392 cost: 1.9605522 error rate: 0.647\n",
      "i: 0 j: 200 nb: 392 cost: 1.9673625 error rate: 0.639\n",
      "i: 0 j: 220 nb: 392 cost: 1.9866122 error rate: 0.639\n",
      "i: 0 j: 240 nb: 392 cost: 1.9514055 error rate: 0.65\n",
      "i: 0 j: 260 nb: 392 cost: 1.8931465 error rate: 0.602\n",
      "i: 0 j: 280 nb: 392 cost: 1.8454106 error rate: 0.585\n",
      "i: 0 j: 300 nb: 392 cost: 1.8311739 error rate: 0.582\n",
      "i: 0 j: 320 nb: 392 cost: 1.8448853 error rate: 0.585\n",
      "i: 0 j: 340 nb: 392 cost: 1.8615756 error rate: 0.614\n",
      "i: 0 j: 360 nb: 392 cost: 1.8942797 error rate: 0.629\n",
      "i: 0 j: 380 nb: 392 cost: 1.8169551 error rate: 0.575\n",
      "i: 1 j: 0 nb: 392 cost: 1.9983687 error rate: 0.692\n",
      "i: 1 j: 20 nb: 392 cost: 1.9050114 error rate: 0.602\n",
      "i: 1 j: 40 nb: 392 cost: 1.938546 error rate: 0.622\n",
      "i: 1 j: 60 nb: 392 cost: 1.9014668 error rate: 0.593\n",
      "i: 1 j: 80 nb: 392 cost: 1.9232347 error rate: 0.59\n",
      "i: 1 j: 100 nb: 392 cost: 1.8945276 error rate: 0.589\n",
      "i: 1 j: 120 nb: 392 cost: 1.926431 error rate: 0.592\n",
      "i: 1 j: 140 nb: 392 cost: 1.928267 error rate: 0.59\n",
      "i: 1 j: 160 nb: 392 cost: 1.9197855 error rate: 0.574\n",
      "i: 1 j: 180 nb: 392 cost: 2.0245101 error rate: 0.619\n",
      "i: 1 j: 200 nb: 392 cost: 2.0139432 error rate: 0.63\n",
      "i: 1 j: 220 nb: 392 cost: 2.0409505 error rate: 0.601\n",
      "i: 1 j: 240 nb: 392 cost: 2.0228643 error rate: 0.614\n",
      "i: 1 j: 260 nb: 392 cost: 2.051556 error rate: 0.608\n",
      "i: 1 j: 280 nb: 392 cost: 2.043848 error rate: 0.582\n",
      "i: 1 j: 300 nb: 392 cost: 2.0045528 error rate: 0.581\n",
      "i: 1 j: 320 nb: 392 cost: 2.0720305 error rate: 0.599\n",
      "i: 1 j: 340 nb: 392 cost: 2.0817125 error rate: 0.603\n",
      "i: 1 j: 360 nb: 392 cost: 2.1165793 error rate: 0.587\n",
      "i: 1 j: 380 nb: 392 cost: 2.2230954 error rate: 0.609\n",
      "i: 2 j: 0 nb: 392 cost: 2.2787414 error rate: 0.634\n",
      "i: 2 j: 20 nb: 392 cost: 2.1843574 error rate: 0.609\n",
      "i: 2 j: 40 nb: 392 cost: 2.2384436 error rate: 0.632\n",
      "i: 2 j: 60 nb: 392 cost: 2.2837312 error rate: 0.638\n",
      "i: 2 j: 80 nb: 392 cost: 2.293313 error rate: 0.596\n",
      "i: 2 j: 100 nb: 392 cost: 2.2287328 error rate: 0.623\n",
      "i: 2 j: 120 nb: 392 cost: 2.1999168 error rate: 0.616\n",
      "i: 2 j: 140 nb: 392 cost: 2.1834328 error rate: 0.582\n",
      "i: 2 j: 160 nb: 392 cost: 2.2237515 error rate: 0.585\n",
      "i: 2 j: 180 nb: 392 cost: 2.1741676 error rate: 0.581\n",
      "i: 2 j: 200 nb: 392 cost: 2.284423 error rate: 0.629\n",
      "i: 2 j: 220 nb: 392 cost: 2.3119004 error rate: 0.595\n",
      "i: 2 j: 240 nb: 392 cost: 2.3236113 error rate: 0.618\n",
      "i: 2 j: 260 nb: 392 cost: 2.3846831 error rate: 0.609\n",
      "i: 2 j: 280 nb: 392 cost: 2.3699646 error rate: 0.629\n",
      "i: 2 j: 300 nb: 392 cost: 2.3643665 error rate: 0.63\n",
      "i: 2 j: 320 nb: 392 cost: 2.4089818 error rate: 0.624\n",
      "i: 2 j: 340 nb: 392 cost: 2.3813384 error rate: 0.638\n",
      "i: 2 j: 360 nb: 392 cost: 2.3846955 error rate: 0.626\n",
      "i: 2 j: 380 nb: 392 cost: 2.4417481 error rate: 0.647\n",
      "i: 3 j: 0 nb: 392 cost: 2.4293904 error rate: 0.643\n",
      "i: 3 j: 20 nb: 392 cost: 2.4519145 error rate: 0.623\n",
      "i: 3 j: 40 nb: 392 cost: 2.5333781 error rate: 0.625\n",
      "i: 3 j: 60 nb: 392 cost: 2.695345 error rate: 0.856\n",
      "i: 3 j: 80 nb: 392 cost: 2.657564 error rate: 0.729\n",
      "i: 3 j: 100 nb: 392 cost: 2.7312653 error rate: 0.755\n",
      "i: 3 j: 120 nb: 392 cost: 2.7389388 error rate: 0.755\n",
      "i: 3 j: 140 nb: 392 cost: 2.752079 error rate: 0.755\n",
      "i: 3 j: 160 nb: 392 cost: 2.7549443 error rate: 0.755\n",
      "i: 3 j: 180 nb: 392 cost: 2.7408082 error rate: 0.755\n",
      "i: 3 j: 200 nb: 392 cost: 2.7332497 error rate: 0.755\n",
      "i: 3 j: 220 nb: 392 cost: 2.7176278 error rate: 0.755\n",
      "i: 3 j: 240 nb: 392 cost: 2.6951296 error rate: 0.755\n",
      "i: 3 j: 260 nb: 392 cost: 2.6508164 error rate: 0.755\n",
      "i: 3 j: 280 nb: 392 cost: 2.6130137 error rate: 0.755\n",
      "i: 3 j: 300 nb: 392 cost: 2.5717337 error rate: 0.755\n",
      "i: 3 j: 320 nb: 392 cost: 2.5383005 error rate: 0.755\n",
      "i: 3 j: 340 nb: 392 cost: 2.5075116 error rate: 0.755\n",
      "i: 3 j: 360 nb: 392 cost: 2.4519925 error rate: 0.755\n",
      "i: 3 j: 380 nb: 392 cost: 2.4062893 error rate: 0.755\n",
      "i: 4 j: 0 nb: 392 cost: 2.3817136 error rate: 0.755\n",
      "i: 4 j: 20 nb: 392 cost: 2.3471947 error rate: 0.755\n",
      "i: 4 j: 40 nb: 392 cost: 2.3152068 error rate: 0.755\n",
      "i: 4 j: 60 nb: 392 cost: 2.2743115 error rate: 0.755\n",
      "i: 4 j: 80 nb: 392 cost: 2.2375534 error rate: 0.755\n",
      "i: 4 j: 100 nb: 392 cost: 2.2040036 error rate: 0.755\n",
      "i: 4 j: 120 nb: 392 cost: 2.1779237 error rate: 0.755\n",
      "i: 4 j: 140 nb: 392 cost: 2.1527226 error rate: 0.755\n",
      "i: 4 j: 160 nb: 392 cost: 2.1240604 error rate: 0.755\n",
      "i: 4 j: 180 nb: 392 cost: 2.1016264 error rate: 0.755\n",
      "i: 4 j: 200 nb: 392 cost: 2.081981 error rate: 0.755\n",
      "i: 4 j: 220 nb: 392 cost: 2.0670784 error rate: 0.755\n",
      "i: 4 j: 240 nb: 392 cost: 2.0482721 error rate: 0.755\n",
      "i: 4 j: 260 nb: 392 cost: 2.0251036 error rate: 0.755\n",
      "i: 4 j: 280 nb: 392 cost: 2.0118515 error rate: 0.755\n",
      "i: 4 j: 300 nb: 392 cost: 1.9970453 error rate: 0.755\n",
      "i: 4 j: 320 nb: 392 cost: 1.9958481 error rate: 0.755\n",
      "i: 4 j: 340 nb: 392 cost: 1.9914513 error rate: 0.755\n",
      "i: 4 j: 360 nb: 392 cost: 1.9644847 error rate: 0.755\n",
      "i: 4 j: 380 nb: 392 cost: 1.958351 error rate: 0.755\n",
      "i: 5 j: 0 nb: 392 cost: 1.9553809 error rate: 0.755\n",
      "i: 5 j: 20 nb: 392 cost: 1.9523846 error rate: 0.755\n",
      "i: 5 j: 40 nb: 392 cost: 1.9465966 error rate: 0.755\n",
      "i: 5 j: 60 nb: 392 cost: 1.9323847 error rate: 0.755\n",
      "i: 5 j: 80 nb: 392 cost: 1.9298594 error rate: 0.755\n",
      "i: 5 j: 100 nb: 392 cost: 1.9291109 error rate: 0.755\n",
      "i: 5 j: 120 nb: 392 cost: 1.9450685 error rate: 0.829\n",
      "i: 5 j: 140 nb: 392 cost: 1.9195328 error rate: 0.755\n",
      "i: 5 j: 160 nb: 392 cost: 1.9219356 error rate: 0.755\n",
      "i: 5 j: 180 nb: 392 cost: 1.9113193 error rate: 0.755\n",
      "i: 5 j: 200 nb: 392 cost: 1.9207127 error rate: 0.755\n",
      "i: 5 j: 220 nb: 392 cost: 1.9141102 error rate: 0.755\n",
      "i: 5 j: 240 nb: 392 cost: 1.9135451 error rate: 0.755\n",
      "i: 5 j: 260 nb: 392 cost: 1.9139363 error rate: 0.755\n",
      "i: 5 j: 280 nb: 392 cost: 1.9157063 error rate: 0.755\n",
      "i: 5 j: 300 nb: 392 cost: 1.902779 error rate: 0.755\n",
      "i: 5 j: 320 nb: 392 cost: 1.9063376 error rate: 0.755\n",
      "i: 5 j: 340 nb: 392 cost: 1.9133538 error rate: 0.755\n",
      "i: 5 j: 360 nb: 392 cost: 1.913333 error rate: 0.755\n",
      "i: 5 j: 380 nb: 392 cost: 1.9017525 error rate: 0.755\n",
      "i: 6 j: 0 nb: 392 cost: 1.8999305 error rate: 0.755\n",
      "i: 6 j: 20 nb: 392 cost: 1.9026684 error rate: 0.755\n",
      "i: 6 j: 40 nb: 392 cost: 1.909558 error rate: 0.755\n",
      "i: 6 j: 60 nb: 392 cost: 1.9066762 error rate: 0.755\n",
      "i: 6 j: 80 nb: 392 cost: 1.9022002 error rate: 0.755\n",
      "i: 6 j: 100 nb: 392 cost: 1.9094566 error rate: 0.755\n",
      "i: 6 j: 120 nb: 392 cost: 1.9134178 error rate: 0.755\n",
      "i: 6 j: 140 nb: 392 cost: 1.9051263 error rate: 0.755\n",
      "i: 6 j: 160 nb: 392 cost: 1.9045924 error rate: 0.755\n",
      "i: 6 j: 180 nb: 392 cost: 1.9074528 error rate: 0.755\n",
      "i: 6 j: 200 nb: 392 cost: 1.9063994 error rate: 0.829\n",
      "i: 6 j: 220 nb: 392 cost: 1.9036857 error rate: 0.755\n",
      "i: 6 j: 240 nb: 392 cost: 1.9085615 error rate: 0.755\n",
      "i: 6 j: 260 nb: 392 cost: 1.9174148 error rate: 0.755\n",
      "i: 6 j: 280 nb: 392 cost: 1.9038682 error rate: 0.755\n",
      "i: 6 j: 300 nb: 392 cost: 1.9033017 error rate: 0.755\n",
      "i: 6 j: 320 nb: 392 cost: 1.9101067 error rate: 0.755\n",
      "i: 6 j: 340 nb: 392 cost: 1.9217501 error rate: 0.755\n",
      "i: 6 j: 360 nb: 392 cost: 1.9019454 error rate: 0.755\n",
      "i: 6 j: 380 nb: 392 cost: 1.8989235 error rate: 0.755\n",
      "i: 7 j: 0 nb: 392 cost: 1.9013731 error rate: 0.755\n",
      "i: 7 j: 20 nb: 392 cost: 1.9067407 error rate: 0.755\n",
      "i: 7 j: 40 nb: 392 cost: 1.9028103 error rate: 0.755\n",
      "i: 7 j: 60 nb: 392 cost: 1.9051477 error rate: 0.755\n",
      "i: 7 j: 80 nb: 392 cost: 1.9236742 error rate: 0.755\n",
      "i: 7 j: 100 nb: 392 cost: 1.913655 error rate: 0.755\n",
      "i: 7 j: 120 nb: 392 cost: 1.9019461 error rate: 0.755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 7 j: 140 nb: 392 cost: 1.9062198 error rate: 0.755\n",
      "i: 7 j: 160 nb: 392 cost: 1.9111195 error rate: 0.755\n",
      "i: 7 j: 180 nb: 392 cost: 1.9032702 error rate: 0.755\n",
      "i: 7 j: 200 nb: 392 cost: 1.9050959 error rate: 0.755\n",
      "i: 7 j: 220 nb: 392 cost: 1.9098127 error rate: 0.755\n",
      "i: 7 j: 240 nb: 392 cost: 1.8999878 error rate: 0.755\n",
      "i: 7 j: 260 nb: 392 cost: 1.9003899 error rate: 0.755\n",
      "i: 7 j: 280 nb: 392 cost: 1.9037112 error rate: 0.755\n",
      "i: 7 j: 300 nb: 392 cost: 1.9218634 error rate: 0.755\n",
      "i: 7 j: 320 nb: 392 cost: 1.903661 error rate: 0.755\n",
      "i: 7 j: 340 nb: 392 cost: 1.8983796 error rate: 0.755\n",
      "i: 7 j: 360 nb: 392 cost: 1.9082354 error rate: 0.755\n",
      "i: 7 j: 380 nb: 392 cost: 1.9091247 error rate: 0.755\n",
      "i: 8 j: 0 nb: 392 cost: 1.9138633 error rate: 0.755\n",
      "i: 8 j: 20 nb: 392 cost: 1.9049928 error rate: 0.755\n",
      "i: 8 j: 40 nb: 392 cost: 1.9048241 error rate: 0.755\n",
      "i: 8 j: 60 nb: 392 cost: 1.9109169 error rate: 0.755\n",
      "i: 8 j: 80 nb: 392 cost: 1.9179018 error rate: 0.755\n",
      "i: 8 j: 100 nb: 392 cost: 1.9173846 error rate: 0.829\n",
      "i: 8 j: 120 nb: 392 cost: 1.9219033 error rate: 0.829\n",
      "i: 8 j: 140 nb: 392 cost: 1.9238573 error rate: 0.755\n",
      "i: 8 j: 160 nb: 392 cost: 1.9288183 error rate: 0.755\n",
      "i: 8 j: 180 nb: 392 cost: 1.9180998 error rate: 0.829\n",
      "i: 8 j: 200 nb: 392 cost: 1.9165897 error rate: 0.755\n",
      "i: 8 j: 220 nb: 392 cost: 1.9086053 error rate: 0.755\n",
      "i: 8 j: 240 nb: 392 cost: 1.9361535 error rate: 0.829\n",
      "i: 8 j: 260 nb: 392 cost: 1.9063822 error rate: 0.755\n",
      "i: 8 j: 280 nb: 392 cost: 1.9177302 error rate: 0.755\n",
      "i: 8 j: 300 nb: 392 cost: 1.9169257 error rate: 0.755\n",
      "i: 8 j: 320 nb: 392 cost: 1.9088992 error rate: 0.755\n",
      "i: 8 j: 340 nb: 392 cost: 1.917593 error rate: 0.755\n",
      "i: 8 j: 360 nb: 392 cost: 1.9043874 error rate: 0.755\n",
      "i: 8 j: 380 nb: 392 cost: 1.9105015 error rate: 0.755\n",
      "i: 9 j: 0 nb: 392 cost: 1.9123945 error rate: 0.755\n",
      "i: 9 j: 20 nb: 392 cost: 1.9127363 error rate: 0.755\n",
      "i: 9 j: 40 nb: 392 cost: 1.9037082 error rate: 0.755\n",
      "i: 9 j: 60 nb: 392 cost: 1.9063053 error rate: 0.755\n",
      "i: 9 j: 80 nb: 392 cost: 1.9214449 error rate: 0.755\n",
      "i: 9 j: 100 nb: 392 cost: 1.9001807 error rate: 0.755\n",
      "i: 9 j: 120 nb: 392 cost: 1.9048642 error rate: 0.755\n",
      "i: 9 j: 140 nb: 392 cost: 1.9186145 error rate: 0.755\n",
      "i: 9 j: 160 nb: 392 cost: 1.910111 error rate: 0.755\n",
      "i: 9 j: 180 nb: 392 cost: 1.9006357 error rate: 0.755\n",
      "i: 9 j: 200 nb: 392 cost: 1.9325802 error rate: 0.859\n",
      "i: 9 j: 220 nb: 392 cost: 1.9034156 error rate: 0.755\n",
      "i: 9 j: 240 nb: 392 cost: 1.9067522 error rate: 0.755\n",
      "i: 9 j: 260 nb: 392 cost: 1.925324 error rate: 0.755\n",
      "i: 9 j: 280 nb: 392 cost: 1.9112699 error rate: 0.755\n",
      "i: 9 j: 300 nb: 392 cost: 1.8996537 error rate: 0.755\n",
      "i: 9 j: 320 nb: 392 cost: 1.933655 error rate: 0.88\n",
      "i: 9 j: 340 nb: 392 cost: 1.939799 error rate: 0.755\n",
      "i: 9 j: 360 nb: 392 cost: 1.9432757 error rate: 0.859\n",
      "i: 9 j: 380 nb: 392 cost: 1.9236051 error rate: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = CNN_TF(\n",
    "    convpool_layer_sizes=[(20, 5, 5), (20, 5, 5)],\n",
    "    hidden_layer_sizes=[500, 300],\n",
    ")\n",
    "\n",
    "model.fit(X, Y, show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
