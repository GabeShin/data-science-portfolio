{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Facial Expression Recognition Library\n",
    "___\n",
    "\n",
    "In this project, I will aspired to libraries such as Sci_kit learn and create a plug-in neural network library\n",
    "\n",
    "This was a Kaggle competition from 2013. I used the data to practice different models on the data. Following is overview of the competition and the data:\n",
    "\n",
    "\n",
    "    One motivation for representation learning is that learning algorithms can design features better and faster than humans can. To this end, we hold this challenge that does not explicitly require that entries use representation learning. Rather, we introduce an entirely new dataset and invite competitors from all related communities to solve it. The dataset for this challenge is a facial expression classification dataset that we have assembled from the internet. Because this is a newly introduced dataset, this contest will see which methods are the easiest to get quickly working on new data.\n",
    "    \n",
    "## Data\n",
    "____\n",
    "\n",
    "    The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "____\n",
    "Comprised of utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias(M1, M2):\n",
    "    # M1: input size\n",
    "    # M2: output size\n",
    "\n",
    "    W = np.random.randn(M1, M2) / np.sqrt(M1 + M2)\n",
    "    # matrix of M1 by M2, randomized initially to Gaussian normal, divided by square root of the fan-in plus fan-out.\n",
    "    b = np.zeros(M2)\n",
    "    # bias initalized as zeros\n",
    "    return W.astype(np.float32), b.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter(shape, poolsz):\n",
    "    # used for convolutional neural network\n",
    "    w = np.random.randn(*shape) * np.sqrt(2) / np.sqrt(np.prod(shape[1:]) + shape[0]*np.prod(shape[2:] / np.prod(poolsz)))\n",
    "    return w.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX / expX.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost functions\n",
    "def sigmoid_cost(T, Y):\n",
    "    # calculates the cross entropy from the definition for sigmoid cost/ binary classification\n",
    "    return -(T*np.log(Y) + (1-T)*np.log(1-Y)).sum()\n",
    "\n",
    "def cost(T, Y):\n",
    "    # general cross entropy function, works for softmax\n",
    "    return -(T*np.log(Y)).sum()\n",
    "\n",
    "def cost2(T, Y):\n",
    "    # same as cost(), just uses the targets to index Y\n",
    "    # instead of multiplying by a large indicator matrix with mostly 0s\n",
    "    N = len(T)\n",
    "    return -np.log(Y[np.arange(N), T]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rate related!\n",
    "def error_rate(targets, predictions):\n",
    "    return np.mean(targets != predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning related functions\n",
    "\n",
    "def y_to_indicator(y):\n",
    "    # convert y into indicator matrix\n",
    "    # size will be N by K\n",
    "    N = len(y)\n",
    "    K = len(set(y))\n",
    "    ind = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "def getData(balance_ones=True):\n",
    "    # get facial expression data\n",
    "    \n",
    "    # images are 48x48 = 2304 size vectors\n",
    "    \n",
    "    #initialize empty list for X and Y\n",
    "    Y = []\n",
    "    X = []\n",
    "    \n",
    "    # open data\n",
    "    first = True\n",
    "    for line in open('input/fer2013.csv'):\n",
    "        # skip first line\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            # first column is labels -> y\n",
    "            Y.append(int(row[0]))      \n",
    "            # second column is space separated pixels\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "\n",
    "    # convert these into Numpy array\n",
    "    # and also normalize the data\n",
    "    X = np.array(X) / 255.0\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    # because we have imbalance class problem, we will balance the class 1. \n",
    "    if balance_ones:\n",
    "        # get all data except class 1\n",
    "        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n",
    "        \n",
    "        # get all class 1 data\n",
    "        X1 = X[Y==1, :]\n",
    "        \n",
    "        # repeat the data 9 times\n",
    "        X1 = np.repeat(X1, 9, axis=0)\n",
    "        \n",
    "        # stack the data for X0 and X1  \n",
    "        # stack the data for Y0 and 1\n",
    "        X = np.vstack([X0, X1])\n",
    "        Y = np.concatenate((Y0, [1]*len(X1)))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def getImageData():\n",
    "    # keep image shape\n",
    "    X, Y = getData()\n",
    "    N, D = X.shape\n",
    "    d = int(np.sqrt(D))\n",
    "    X = X.reshape(N, 1, d, d)\n",
    "    return X, Y\n",
    "\n",
    "def getBinaryData():\n",
    "    # same as getData function, except we only get binary data/ Y = 0, 1\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    for line in open('input/fer2013.csv'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            y = int(row[0])\n",
    "            if y == 0 or y == 1:\n",
    "                Y.append(y)\n",
    "                X.append([int(p) for p in row[1].split()])\n",
    "    return np.array(X) / 255.0, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Artificial Neural Network for Binary Classification using ReLU and tanh\n",
    "___\n",
    "Create ANN class for facial expression recognition.\n",
    "- Binary Classification (only between 0 and 1)\n",
    "- One hidden layer\n",
    "- uses relu and tanh as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=5e-6, reg=1.0, epochs=10000, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        N, D = X.shape\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        self.W2 = np.random.randn(self.M) / np.sqrt(self.M)\n",
    "        self.b2 = 0\n",
    "\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation and cost calculation\n",
    "            pY, Z = self.forward(X)            \n",
    "\n",
    "            # back propagation - gradient descent step\n",
    "            \n",
    "            # hidden-to-outer\n",
    "            pY_Y = pY - Y # prediction minus the target\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_Y) + reg*self.W2) # update hidden-to-output weight\n",
    "            self.b2 -= learning_rate*((pY_Y).sum() + reg*self.b2) # update hidden-out-output bias\n",
    "\n",
    "            # print \"(pY_Y).dot(self.W2.T) shape:\", (pY_Y).dot(self.W2.T).shape\n",
    "            # print \"Z shape:\", Z.shape\n",
    "\n",
    "            # input-to-hidden\n",
    "            dZ = np.outer(pY_Y, self.W2) * (1 - Z*Z) # tanh\n",
    "            # dZ = np.outer(pY_Y, self.W2) * (Z > 0) # ReLU\n",
    "            \n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1) # update input-to-hidden weight\n",
    "            self.b1 -= learning_rate*(np.sum(dZ, axis=0) + reg*self.b1) # update input-to-hidden bias\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "\n",
    "                c = sigmoid_cost(Yvalid, pYvalid)\n",
    "\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.round(pYvalid))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        print(\"validation score: \", self.score(Xvalid, Yvalid))\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1) # tanh\n",
    "        # Z = relu(X.dot(self.W1) + self.b1) # ReLU\n",
    "\n",
    "        return sigmoid(Z.dot(self.W2) + self.b2), Z\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY = self.forward(X)\n",
    "        return np.round(pY)\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "\n",
    "#     def main():\n",
    "#         X, Y = getBinaryData()\n",
    "\n",
    "#         X0 = X[Y==0, :]\n",
    "#         X1 = X[Y==1, :]\n",
    "#         X1 = np.repeat(X1, 9, axis=0)\n",
    "#         X = np.vstack([X0, X1])\n",
    "#         Y = np.array([0]*len(X0) + [1]*len(X1))\n",
    "\n",
    "#         model = ANN(100)\n",
    "#         model.fit(X, Y, show_fig=True)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Using ANN class!\n",
    "X, Y = getBinaryData()\n",
    "\n",
    "# balance imbalance problem in the data\n",
    "X0 = X[Y==0, :]\n",
    "X1 = X[Y==1, :]\n",
    "# repeat class 1, 9 times\n",
    "X1 = np.repeat(X1, 9, axis=0)\n",
    "# stack X and Y\n",
    "X = np.vstack([X0, X1])\n",
    "Y = np.array([0]*len(X0) + [1]*len(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 712.8074841887204 error: 0.505\n",
      "i: 20 cost: 675.4204328510997 error: 0.375\n",
      "i: 40 cost: 673.4038044929923 error: 0.444\n",
      "i: 60 cost: 674.8167300140576 error: 0.449\n",
      "i: 80 cost: 660.8945024117224 error: 0.403\n",
      "i: 100 cost: 658.1536637345562 error: 0.394\n",
      "i: 120 cost: 654.0929366817682 error: 0.376\n",
      "i: 140 cost: 650.7621580399922 error: 0.371\n",
      "i: 160 cost: 647.8151636201803 error: 0.365\n",
      "i: 180 cost: 645.1117648145158 error: 0.363\n",
      "i: 200 cost: 642.6083010292409 error: 0.363\n",
      "i: 220 cost: 640.2769094804295 error: 0.359\n",
      "i: 240 cost: 638.091964389127 error: 0.355\n",
      "i: 260 cost: 636.0288398245254 error: 0.356\n",
      "i: 280 cost: 634.0666322335646 error: 0.356\n",
      "i: 300 cost: 632.1883163007485 error: 0.355\n",
      "i: 320 cost: 630.3800398017194 error: 0.35\n",
      "i: 340 cost: 628.6304962047142 error: 0.346\n",
      "i: 360 cost: 626.9304523100462 error: 0.345\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fb15ab2875e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fit the data into the model, use tanh activation. ** note: after testing few times, relu activation function doesn't behave well in this model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_fig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4eb17b51d7de>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, learning_rate, reg, epochs, show_fig)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# forward propagation and cost calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mpY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# back propagation - gradient descent step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4eb17b51d7de>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tanh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;31m# Z = relu(X.dot(self.W1) + self.b1) # ReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = ANN(100) # hidden layer size to 100\n",
    "\n",
    "# fit the data into the model, use tanh activation. ** note: after testing few times, relu activation function doesn't behave well in this model.\n",
    "model.fit(X , Y, show_fig=True, epochs = 10000)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Multiple Classfication with Softmax\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNwithSoftmax(object):\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=10e-6, reg=10e-1, epochs=10000, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        # Tvalid = y2indicator(Yvalid)\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        T = y_to_indicator(Y)\n",
    "        \n",
    "        # input-to-hidden layer\n",
    "        self.W1 = np.random.randn(D, self.M) / np.sqrt(D)\n",
    "        self.b1 = np.zeros(self.M)\n",
    "        \n",
    "        # hidden-to-output layer\n",
    "        self.W2 = np.random.randn(self.M, K) / np.sqrt(self.M)\n",
    "        self.b2 = np.zeros(K)\n",
    "\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for i in range(epochs):\n",
    "            # forward propagation and cost calculation\n",
    "            pY, Z = self.forward(X)\n",
    "\n",
    "            # gradient descent step\n",
    "            pY_T = pY - T\n",
    "            self.W2 -= learning_rate*(Z.T.dot(pY_T) + reg*self.W2)\n",
    "            self.b2 -= learning_rate*(pY_T.sum(axis=0) + reg*self.b2)\n",
    "            \n",
    "            # dZ = pY_T.dot(self.W2.T) * (Z > 0) # relu\n",
    "            dZ = pY_T.dot(self.W2.T) * (1 - Z*Z) # tanh\n",
    "            \n",
    "            self.W1 -= learning_rate*(X.T.dot(dZ) + reg*self.W1)\n",
    "            self.b1 -= learning_rate*(dZ.sum(axis=0) + reg*self.b1)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                pYvalid, _ = self.forward(Xvalid)\n",
    "                c = cost2(Yvalid, pYvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(pYvalid, axis=1))\n",
    "                print(\"i:\", i, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)\n",
    "        print(\"validation score: \", self.score(Xvalid, Yvalid))\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Z = relu(X.dot(self.W1) + self.b1)\n",
    "        Z = np.tanh(X.dot(self.W1) + self.b1)\n",
    "        return softmax(Z.dot(self.W2) + self.b2), Z\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY, _ = self.forward(X)\n",
    "        return np.argmax(pY, axis=1)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        prediction = self.predict(X)\n",
    "        return 1 - error_rate(Y, prediction)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     X, Y = getData()\n",
    "    \n",
    "#     model = ANN(200)\n",
    "#     model.fit(X, Y, reg=0, show_fig=True)\n",
    "#     print(model.score(X, Y))\n",
    "#     # scores = cross_val_score(model, X, Y, cv=5)\n",
    "#     # print \"score mean:\", np.mean(scores), \"stdev:\", np.std(scores)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANNwithSoftmax(200)\n",
    "model.fit(X, Y, reg=0, show_fig=True)\n",
    "print(model.score(X, Y))\n",
    "# scores = cross_val_score(model, X, Y, cv=5)\n",
    "# print \"score mean:\", np.mean(scores), \"stdev:\", np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates on different methods coming soon..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
